import itertools
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.ops as ops
from torchvision.models.resnet import resnet50, resnet101

import utils
from config import Config


def _resnet_encoder_stages(arch: str, weight: str = 'DEFAULT'):
    import os
    assert arch in ['resnet50', 'resnet101']
    assert weight in ['DEFAULT', 'IMAGENET1K_V1', 'IMAGENET1K_V2']
    os.environ['TORCH_HOME'] = '/root/autodl-tmp/torch'
    model = resnet50(weights=weight) if arch == 'resnet50' \
        else resnet101(weights=weight)
    stagets = [
        nn.Sequential(model.conv1, model.bn1, model.relu, model.maxpool),
        model.layer1,
        model.layer2,
        model.layer3,
        model.layer4,
    ]
    return stagets


class FPN(nn.Module):
    def __init__(self, backbone: str, out_channels: int, weight: str = 'DEFAULT'):
        super(FPN, self).__init__()
        stages = _resnet_encoder_stages(backbone, weight)
        self.out_channels = out_channels
        self.C1 = stages[0]
        self.C2 = stages[1]
        self.C3 = stages[2]
        self.C4 = stages[3]
        self.C5 = stages[4]

        self.P6 = nn.MaxPool2d(kernel_size=1, stride=2)

        self.P5_conv1 = nn.Conv2d(2048, self.out_channels, kernel_size=1, stride=1)
        self.P5_conv2 = nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, stride=1, padding=1)

        self.P4_conv1 = nn.Conv2d(1024, self.out_channels, kernel_size=1, stride=1)
        self.P4_conv2 = nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, stride=1, padding=1)

        self.P3_conv1 = nn.Conv2d(512, self.out_channels, kernel_size=1, stride=1)
        self.P3_conv2 = nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, stride=1, padding=1)

        self.P2_conv1 = nn.Conv2d(256, self.out_channels, kernel_size=1, stride=1)
        self.P2_conv2 = nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, stride=1, padding=1)

    def init_weights(self):
        """Initialize weights except backbone.
        """
        for m in [self.P2_conv1, self.P2_conv2,
                  self.P3_conv1, self.P3_conv2,
                  self.P4_conv1, self.P4_conv2,
                  self.P5_conv1, self.P5_conv2]:
            nn.init.xavier_uniform_(m.weight)
            if m.bias is not None:
                m.bias.data.zero_()

    def forward(self, x):
        c1_out = self.C1(x)
        c2_out = self.C2(c1_out)
        c3_out = self.C3(c2_out)
        c4_out = self.C4(c3_out)
        c5_out = self.C5(c4_out)

        p5_out = self.P5_conv1(c5_out)
        p4_out = self.P4_conv1(c4_out) + F.interpolate(p5_out, scale_factor=2, mode='bilinear')
        p3_out = self.P3_conv1(c3_out) + F.interpolate(p4_out, scale_factor=2, mode='bilinear')
        p2_out = self.P2_conv1(c2_out) + F.interpolate(p3_out, scale_factor=2, mode='bilinear')

        p5_out = self.P5_conv2(p5_out)
        p4_out = self.P4_conv2(p4_out)
        p3_out = self.P3_conv2(p3_out)
        p2_out = self.P2_conv2(p2_out)

        # P6 is used for the 5th anchor scale in RPN. Generated by
        # subsampling from P5 with stride of 2.
        p6_out = self.P6(p5_out)

        return p2_out, p3_out, p4_out, p5_out, p6_out


class RPN(nn.Module):
    """Builds the model of Region Proposal Network.

    anchors_per_location: number of anchors per pixel in the feature map
    anchor_stride: Controls the density of anchors. Typically 1 (anchors for
                   every pixel in the feature map), or 2 (every other pixel).

    Returns:
        rpn_logits: [batch, H, W, 2] Anchor classifier logits (before softmax)
        rpn_probs: [batch, W, W, 2] Anchor classifier probabilities.
        rpn_bbox: [batch, H, W, (dy, dx, log(dh), log(dw))] Deltas to be
                  applied to anchors.
    """

    def __init__(self, anchors_per_location, anchor_stride, depth):
        super(RPN, self).__init__()
        self.anchors_per_location = anchors_per_location
        self.anchor_stride = anchor_stride
        self.depth = depth

        self.conv_shared = nn.Conv2d(self.depth, 512, kernel_size=3, stride=self.anchor_stride, padding=1)
        self.relu = nn.ReLU(inplace=True)
        self.conv_class = nn.Conv2d(512, 2 * anchors_per_location, kernel_size=1, stride=1)
        self.softmax = nn.Softmax(dim=2)
        self.conv_bbox = nn.Conv2d(512, 4 * anchors_per_location, kernel_size=1, stride=1)

    def forward(self, x):
        # Shared convolutional base of the RPN
        x = self.relu(self.conv_shared(x))

        # Anchor Score. [batch, anchors per location * 2, height, width].
        rpn_logits = self.conv_class(x)

        # Reshape to [batch, 2, anchors]
        rpn_logits = rpn_logits.permute(0, 2, 3, 1)
        rpn_logits = rpn_logits.contiguous()
        rpn_logits = rpn_logits.view(x.size(0), -1, 2)

        # Softmax on last dimension of BG/FG.
        rpn_probs = self.softmax(rpn_logits)

        # Bounding box refinement. [batch, H, W, anchors per location, depth]
        # where depth is [dx, dy, log(dw), log(dh)]
        rpn_deltas = self.conv_bbox(x)

        # Reshape to [batch, 4, anchors]
        rpn_deltas = rpn_deltas.permute(0, 2, 3, 1)
        rpn_deltas = rpn_deltas.contiguous()
        rpn_deltas = rpn_deltas.view(x.size(0), -1, 4)

        return [rpn_logits, rpn_probs, rpn_deltas]


def pyramid_roi_align(inputs, pool_size, image_shape):
    """Implements ROI Pooling on multiple levels of the feature pyramid.

    Params:
    - pool_size: [height, width] of the output pooled regions. Usually [7, 7]
    - image_shape: [height, width, channels]. Shape of input image in pixels

    Inputs:
    - rois: [batch, num_boxes, (x1, y1, x2, y2)] in normalized
             coordinates.
    - Feature maps: List of feature maps from different levels of the pyramid.
                    Each is [batch, channels, height, width]

    Output:
    Pooled regions in the shape: [batch*num_boxes, channels, pool_size, pool_size].
    """
    _device = inputs[1].device
    # Feature Maps. List of feature maps from different level of the
    # feature pyramid. Each is [batch, channels, height, width]
    feature_maps = inputs[0]
    # Crop rois [batch, num_boxes, (x1, y1, x2, y2)] in normalized coords
    rois = inputs[1]

    pooled = []
    bs = rois.size(0)
    for b in range(bs):

        # Assign each ROI to a level in the pyramid based on the ROI area.
        x1, y1, x2, y2 = rois[b].chunk(4, dim=1)
        w = x2 - x1
        h = y2 - y1

        # Equation 1 in the Feature Pyramid Networks paper. Account for
        # the fact that our coordinates are normalized here.
        # e.g. a 224x224 ROI (in pixels) maps to P4
        roi_level = 4 + torch.log2(torch.sqrt(w * h) / \
                                   (224.0 / (image_shape[0] * image_shape[1]) ** 0.5))
        roi_level = roi_level.round().int()
        roi_level = roi_level.clamp(2, 5)

        # Loop through levels and apply ROI pooling to each. P2 to P5.
        pooled_batch = torch.zeros(
            (roi_level.size(0), feature_maps[0].size(1), pool_size, pool_size)).float().to(_device)
        for i, level in enumerate(range(2, 6)):
            ix = roi_level == level
            if not ix.any():
                continue
            ix = torch.nonzero(ix)[:, 0]
            level_rois = rois[b, ix, :]

            # Stop gradient propogation to ROI proposals
            level_rois = level_rois.detach()

            # RoI Align
            # From Mask R-CNN paper: "We sample four regular locations, so
            # that we can evaluate either max or average pooling. In fact,
            # interpolating only a single value at each bin center (without
            # pooling) is nearly as effective."
            #
            # Here we use the simplified approach of a single value per bin,
            # which is how it's done in torchvision.ops.roi_align()
            # Result: [batch * num_boxes, channels, pool_height, pool_width]
            ind = torch.full((level_rois.size(0), 1), b).int().to(_device)
            pooled_features = ops.roi_align(feature_maps[i],
                                            torch.cat([ind, level_rois], dim=1),
                                            output_size=pool_size,
                                            spatial_scale=feature_maps[i].size(2))
            pooled_batch[ix, :, :, :] = pooled_features

        pooled.append(pooled_batch)
    # Pack pooled features into one tensor
    pooled = torch.cat(pooled, dim=0)

    return pooled


class Classifier(nn.Module):
    def __init__(self, depth, pool_size, image_shape, num_classes):
        super(Classifier, self).__init__()
        self.depth = depth
        self.pool_size = pool_size
        self.image_shape = image_shape
        self.num_classes = num_classes
        self.conv1 = nn.Conv2d(self.depth, 1024, kernel_size=self.pool_size, stride=1)
        self.bn1 = nn.BatchNorm2d(1024)
        self.conv2 = nn.Conv2d(1024, 1024, kernel_size=1, stride=1)
        self.bn2 = nn.BatchNorm2d(1024)
        self.relu = nn.ReLU(inplace=True)

        self.linear_class = nn.Linear(1024, num_classes)
        self.softmax = nn.Softmax(dim=2)

        self.linear_bbox = nn.Linear(1024, num_classes * 4)

    def forward(self, x, rois):
        bs, num_rois = rois.size()[:2]
        x = pyramid_roi_align([x, rois], self.pool_size, self.image_shape)
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.conv2(x)
        x = self.bn2(x)
        x = self.relu(x)

        x = x.view(-1, 1024)
        mrcnn_class_logits = self.linear_class(x)
        mrcnn_class_logits = mrcnn_class_logits.view(bs, num_rois, self.num_classes)
        mrcnn_class_probs = self.softmax(mrcnn_class_logits)

        mrcnn_deltas = self.linear_bbox(x)
        mrcnn_deltas = mrcnn_deltas.view(bs, num_rois, self.num_classes, 4)

        return mrcnn_class_logits, mrcnn_class_probs, mrcnn_deltas


class Mask(nn.Module):
    def __init__(self, depth, pool_size, image_shape, num_classes):
        super(Mask, self).__init__()
        self.depth = depth
        self.pool_size = pool_size
        self.image_shape = image_shape
        self.num_classes = num_classes
        self.conv1 = nn.Conv2d(self.depth, 256, kernel_size=3, stride=1, padding=1)
        self.bn1 = nn.BatchNorm2d(256)
        self.conv2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)
        self.bn2 = nn.BatchNorm2d(256)
        self.conv3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)
        self.bn3 = nn.BatchNorm2d(256)
        self.conv4 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)
        self.bn4 = nn.BatchNorm2d(256)
        self.deconv = nn.ConvTranspose2d(256, 256, kernel_size=2, stride=2)
        self.conv5 = nn.Conv2d(256, num_classes, kernel_size=1, stride=1)
        self.sigmoid = nn.Sigmoid()
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x, rois):
        bs, num_rois = rois.size()[:2]
        x = pyramid_roi_align([x, rois], self.pool_size, self.image_shape)
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.conv2(x)
        x = self.bn2(x)
        x = self.relu(x)
        x = self.conv3(x)
        x = self.bn3(x)
        x = self.relu(x)
        x = self.conv4(x)
        x = self.bn4(x)
        x = self.relu(x)
        x = self.deconv(x)
        x = self.relu(x)
        x = self.conv5(x)
        x = self.sigmoid(x)

        x = x.view(bs, num_rois, self.num_classes, x.size(-2), x.size(-1))

        return x


def proposal_layer(inputs, anchors, nms_threshold, proposal_count, config):
    """Receives anchor scores and selects a subset to pass as proposals
    to the second stage. Filtering is done based on anchor scores and
    non-max suppression to remove overlaps. It also applies bounding
    box refinment detals to anchors.

    Inputs:
        rpn_probs: [batch, anchors, (bg prob, fg prob)]
        rpn_deltas: [batch, anchors, (dx, dy, log(dw), log(dh))]

    Returns:
        Proposals in normalized coordinates [batch, rois, (x1, y1, x2, y2)]
    """
    _device = inputs[0].device

    # Box Scores. Use the foreground class confidence. [Batch, num_rois]
    scores = inputs[0][:, :, 1]

    # Box deltas [batch, num_rois, 4]
    deltas = inputs[1]
    std_dev = torch.tensor(config.RPN_BBOX_STD_DEV).float().to(deltas.device)
    deltas = deltas * std_dev

    # Improve performance by trimming to top anchors by score
    # and doing the rest on the smaller subset.
    pre_nms_limit = min(6000, anchors.size(0))
    scores, order = scores.sort(dim=1, descending=True)
    order = order[:, :pre_nms_limit]
    scores = scores[:, :pre_nms_limit]
    deltas = deltas.gather(1, order.unsqueeze(2).repeat(1, 1, 4))
    anchors = anchors[order]

    # Apply deltas to anchors to get refined anchors.
    # [batch, N, (x1, y1, x2, y2)]
    bboxes = utils.apply_box_deltas(anchors, deltas)

    # Clip to image boundaries. [batch, N, (x1, y1, x2, y2)]
    height, width = config.IMAGE_SHAPE[:2]
    window = np.array([0, 0, width, height]).astype(np.float32)
    bboxes = utils.clip_boxes(bboxes, window)

    # Non-max suppression
    filtered_bboxes = torch.zeros((bboxes.size(0), proposal_count, 4)).float().to(_device)
    for i in range(bboxes.size(0)):
        keep = ops.nms(bboxes[i], scores[i], nms_threshold)
        keep = keep[:proposal_count]
        filtered_bboxes[i, :len(keep), :] = bboxes[i, keep, :]

    # Normalize dimensions to range of 0 to 1.
    norm = torch.tensor([width, height, width, height]).float().to(_device)
    proposals = filtered_bboxes / norm

    return proposals


def detection_target_layer(proposals, gt_class_ids, gt_bboxes, gt_masks, config):
    """Subsamples proposals and generates target box refinment, class_ids,
    and masks for each.

    Inputs:
    proposals: [batch, N, (x1, y1, x2, y2)] in normalized coordinates. Might
               be zero padded if there are not enough proposals.
    gt_class_ids: [batch, MAX_GT_INSTANCES] Integer class IDs.
    gt_bboxes: [batch, MAX_GT_INSTANCES, (x1, y1, x2, y2)] in normalized
              coordinates.
    gt_masks: [batch, MAX_GT_INSTANCES, height, width] of float type

    Returns: Target ROIs and corresponding class IDs, bounding box shifts,
    and masks.
    rois: [batch, TRAIN_ROIS_PER_IMAGE, (x1, y1, x2, y2)] in normalized
          coordinates
    target_class_ids: [batch, TRAIN_ROIS_PER_IMAGE]. Integer class IDs.
    target_deltas: [batch, TRAIN_ROIS_PER_IMAGE, (dx, dy, log(dw), log(dh))]
                    Bbox refinments.
    target_masks: [batch, TRAIN_ROIS_PER_IMAGE, height, width)
                 Masks cropped to bbox boundaries and resized to neural
                 network output size.
    """
    _device = proposals.device
    bs = proposals.size(0)
    rois = torch.zeros((bs, config.TRAIN_ROIS_PER_IMAGE, 4)).float().to(_device)
    target_class_ids = torch.zeros((bs, config.TRAIN_ROIS_PER_IMAGE)).int().to(_device)
    target_deltas = torch.zeros((bs, config.TRAIN_ROIS_PER_IMAGE, 4)).float().to(_device)
    target_masks = torch.zeros(
        (bs, config.TRAIN_ROIS_PER_IMAGE, config.MASK_SHAPE[0], config.MASK_SHAPE[1])).float().to(_device)

    for i in range(bs):
        # Compute overlaps matrix [proposals, gt_bboxes]
        overlaps = ops.box_iou(proposals[i], gt_bboxes[i])

        # Determine postive and negative ROIs
        roi_iou_max = torch.max(overlaps, dim=1)[0]

        # 1. Positive ROIs are those with >= 0.5 IoU with a GT box
        positive_roi_bool = roi_iou_max >= 0.5

        # Subsample ROIs. Aim for 33% positive
        # Positive ROIs
        if torch.nonzero(positive_roi_bool).size(0):
            positive_indices = torch.nonzero(positive_roi_bool)[:, 0]

            positive_count = int(config.TRAIN_ROIS_PER_IMAGE *
                                 config.ROI_POSITIVE_RATIO)
            rand_idx = torch.randperm(positive_indices.size(0)).to(_device)
            rand_idx = rand_idx[:positive_count]
            positive_indices = positive_indices[rand_idx]
            positive_count = positive_indices.size(0)
            positive_rois = proposals[i, positive_indices, :]

            # Assign positive ROIs to GT boxes.
            positive_overlaps = overlaps[positive_indices, :]
            roi_assignment = torch.max(positive_overlaps, dim=1)[1]
            roi_bboxes = gt_bboxes[i, roi_assignment, :]
            roi_class_ids = gt_class_ids[i, roi_assignment]

            # Compute bbox refinement for positive ROIs
            roi_deltas = utils.box_refinement(positive_rois, roi_bboxes)
            std_dev = torch.tensor(config.BBOX_STD_DEV).float().to(_device)
            roi_deltas /= std_dev

            # Assign positive ROIs to GT masks
            roi_masks = gt_masks[i, roi_assignment, :, :]

            # Compute mask targets
            boxes = positive_rois
            if config.USE_MINI_MASK:
                # Transform ROI corrdinates from normalized image space
                # to normalized mini-mask space.
                x1, y1, x2, y2 = positive_rois.chunk(4, dim=1)
                gt_x1, gt_y1, gt_x2, gt_y2 = roi_bboxes.chunk(4, dim=1)
                gt_w = gt_x2 - gt_x1
                gt_h = gt_y2 - gt_y1
                x1 = (x1 - gt_x1) / gt_w
                y1 = (y1 - gt_y1) / gt_h
                x2 = (x2 - gt_x1) / gt_w
                y2 = (y2 - gt_y1) / gt_h
                boxes = torch.cat([x1, y1, x2, y2], dim=1)
            box_ids = torch.arange(roi_masks.size(0)).reshape((-1, 1)).float().to(_device)
            roi_masks = ops.roi_align(roi_masks.unsqueeze(1),
                                      torch.cat([box_ids, boxes], dim=1),
                                      output_size=config.MASK_SHAPE,
                                      spatial_scale=roi_masks.size(-1))
            roi_masks = roi_masks.squeeze(1)
            # Threshold mask pixels at 0.5 to have GT masks be 0 or 1 to use with
            # binary cross entropy loss.
            roi_masks = torch.round(roi_masks)

            rois[i, :positive_count, :] = positive_rois
            target_class_ids[i, :positive_count] = roi_class_ids
            target_deltas[i, :positive_count, :] = roi_deltas
            target_masks[i, :positive_count, :, :] = roi_masks
        else:
            positive_count = 0

        # 2. Negative ROIs are those with < 0.5 with every GT box.
        negative_roi_bool = roi_iou_max < 0.5
        # Negative ROIs. Add enough to maintain positive:negative ratio.
        if torch.nonzero(negative_roi_bool).size(0):
            negative_indices = torch.nonzero(negative_roi_bool)[:, 0]
            negative_count = config.TRAIN_ROIS_PER_IMAGE - positive_count
            rand_idx = torch.randperm(negative_indices.size(0)).to(_device)
            rand_idx = rand_idx[:negative_count]
            negative_indices = negative_indices[rand_idx]
            negative_count = negative_indices.size(0)
            rois[i, positive_count:positive_count + negative_count, :] = proposals[i, negative_indices, :]

    return rois, target_class_ids, target_deltas, target_masks


def detection_layer(proposals, mrcnn_probs, mrcnn_deltas, config):
    """Takes classified proposal boxes and their bounding box deltas and
    returns the final detection boxes.

    Returns:
    [batch, num_detections, (x1, y1, x2, y2, class_id, score)] in pixels
    """
    _device = proposals.device
    bs = proposals.size(0)
    detections = torch.zeros((bs, config.DETECTION_MAX_INSTANCES, 6)).float().to(_device)

    for b in range(bs):
        # Class IDs per ROI
        # Class probability of the top class of each ROI
        scores, class_ids = torch.max(mrcnn_probs[b], dim=1)

        # Class-specific bounding box deltas
        idx = torch.arange(class_ids.size(0)).long().to(_device)
        deltas = mrcnn_deltas[b, idx, class_ids]

        # Apply bounding box deltas
        # Shape: [boxes, (x1, y1, x2, y2)] in normalized coordinates
        std_dev = torch.tensor(config.BBOX_STD_DEV).float().to(_device)
        bboxes = utils.apply_box_deltas(proposals[b], deltas * std_dev)
        # Clip boxes to image window
        bboxes = utils.clip_boxes(bboxes, [0, 0, 1, 1])

        # Filter out background boxes
        keep_bool = class_ids > 0
        # Filter out low confidence boxes
        if config.DETECTION_MIN_CONFIDENCE:
            keep_bool = keep_bool & (scores >= config.DETECTION_MIN_CONFIDENCE)
        keep = torch.nonzero(keep_bool)[:, 0]

        # Apply per-class NMS
        pre_nms_class_ids = class_ids[keep]
        pre_nms_scores = scores[keep]
        pre_nms_bboxes = bboxes[keep]
        nms_keep = torch.tensor([]).long().to(_device)
        for class_id in torch.unique(pre_nms_class_ids):
            # Pick detections of this class
            ixs = torch.nonzero(pre_nms_class_ids == class_id)[:, 0]
            ix_bboxes = pre_nms_bboxes[ixs, :]
            ix_scores = pre_nms_scores[ixs]
            class_keep = ops.nms(ix_bboxes, ix_scores, config.DETECTION_NMS_THRESHOLD)

            # Map indicies
            class_keep = keep[ixs[class_keep]]

            nms_keep = torch.cat((nms_keep, class_keep))
        keep = nms_keep

        # Keep top detections
        top_ids = scores[keep].sort(descending=True)[1][:config.DETECTION_MAX_INSTANCES]
        keep = keep[top_ids]

        # Arrange output as [N, (x1, y1, x2, y2, class_id, score)]
        # Coordinates are normalized.
        detection = torch.cat((bboxes[keep],
                               class_ids[keep].unsqueeze(1).float(),
                               scores[keep].unsqueeze(1)), dim=1)
        detections[b, :detection.size(0), :] = detection

    return detections


class MaskRCNN(nn.Module):
    """Encapsulates the Mask RCNN model functionality.
    """

    def __init__(self, config: Config):
        super(MaskRCNN, self).__init__()
        self.config = config

        # Image size must be dividable by 2 multiple times
        h, w = config.IMAGE_SHAPE[:2]
        if h / 2 ** 6 != int(h / 2 ** 6) or w / 2 ** 6 != int(w / 2 ** 6):
            raise Exception("Image size must be dividable by 2 at least 6 times "
                            "to avoid fractions when downscaling and upscaling."
                            "For example, use 256, 320, 384, 448, 512, ... etc. ")

        # Generate Anchors
        self.anchors = torch.tensor(utils.generate_pyramid_anchors(
            config.RPN_ANCHOR_SCALES,
            config.RPN_ANCHOR_RATIOS,
            config.BACKBONE_SHAPES,
            config.BACKBONE_STRIDES,
            config.RPN_ANCHOR_STRIDE
        )).float()

        # FPN
        self.fpn = FPN(config.BACKBONE_ARCH, out_channels=256, weight=config.BACKBONE_INIT_WEIGHT)

        # RPN
        self.rpn = RPN(len(config.RPN_ANCHOR_RATIOS), config.RPN_ANCHOR_STRIDE, 256)

        # FPN Classifier
        self.classifier = Classifier(256, config.POOL_SIZE, config.IMAGE_SHAPE, config.NUM_CLASSES)

        # FPN Mask
        self.mask = Mask(256, config.MASK_POOL_SIZE, config.IMAGE_SHAPE, config.NUM_CLASSES)

        # Fix batch norm layers
        def set_bn_fix(m):
            classname = m.__class__.__name__
            if classname.find('BatchNorm') != -1:
                for p in m.parameters():
                    p.requires_grad = False

        self.apply(set_bn_fix)

        self.init_weight()

    def init_weight(self):
        """Initialize model weights.
        """
        self.fpn.init_weights()
        for m in itertools.chain(self.rpn.modules(), self.classifier.modules(), self.mask.modules()):
            if isinstance(m, nn.Conv2d):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    m.bias.data.zero_()
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()
            elif isinstance(m, nn.Linear):
                m.weight.data.normal_(0, 0.01)
                m.bias.data.zero_()

    def forward(self, inputs):
        images = inputs[0]
        if images.is_cuda and not self.anchors.is_cuda:
            self.anchors = self.anchors.to(images.device)

        # Set batchnorm always in eval mode during training
        if self.training:
            def set_bn_eval(m):
                classname = m.__class__.__name__
                if classname.find('BatchNorm') != -1:
                    m.eval()

            self.apply(set_bn_eval)

        # Feature extraction
        p2_out, p3_out, p4_out, p5_out, p6_out = self.fpn(images)

        # Note that P6 is used in RPN, but not in the classifier heads.
        rpn_feature_maps = [p2_out, p3_out, p4_out, p5_out, p6_out]
        mrcnn_feature_maps = [p2_out, p3_out, p4_out, p5_out]

        # Loop through pyramid layers
        layer_outputs = []  # list of lists
        for p in rpn_feature_maps:
            layer_outputs.append(self.rpn(p))

        # Concatenate layer outputs
        # Convert from list of lists of level outputs to list of lists
        # of outputs across levels.
        # e.g. [[a1, b1, c1], [a2, b2, c2]] => [[a1, a2], [b1, b2], [c1, c2]]
        outputs = list(zip(*layer_outputs))
        outputs = [torch.cat(list(o), dim=1) for o in outputs]
        rpn_logits, rpn_probs, rpn_deltas = outputs

        # Generate proposals
        # Proposals are [batch, N, (x1, y1, x2, y2)] in normalized coordinates
        # and zero padded.
        proposal_count = self.config.POST_NMS_ROIS_TRAINING if self.training \
            else self.config.POST_NMS_ROIS_INFERENCE
        proposals = proposal_layer([rpn_probs, rpn_deltas],
                                   anchors=self.anchors,
                                   nms_threshold=self.config.RPN_NMS_THRESHOLD,
                                   proposal_count=proposal_count,
                                   config=self.config)

        if self.training:
            gt_class_ids = inputs[1]
            gt_bboxes = inputs[2]
            gt_masks = inputs[3]

            # Normalize coordinates
            h, w = self.config.IMAGE_SHAPE[:2]
            scale = torch.tensor(np.array([w, h, w, h])).float().to(gt_bboxes.device)
            gt_bboxes = gt_bboxes / scale

            # Generate detection targets
            # Subsamples proposals and generates target outputs for training
            proposals, target_class_ids, target_deltas, target_masks = \
                detection_target_layer(proposals, gt_class_ids, gt_bboxes, gt_masks, self.config)

            # Network Heads
            # Proposal classifier and BBox regressor heads
            mrcnn_class_logits, _, mrcnn_deltas = self.classifier(mrcnn_feature_maps, proposals)

            # Create masks for detections
            mrcnn_masks = self.mask(mrcnn_feature_maps, proposals)

            return rpn_logits, rpn_deltas, \
                target_class_ids, mrcnn_class_logits, \
                target_deltas, mrcnn_deltas, \
                target_masks, mrcnn_masks
        else:
            # Network Heads
            # Proposal classifier and BBox regressor heads
            _, mrcnn_class_probs, mrcnn_deltas = self.classifier(mrcnn_feature_maps, proposals)

            # Detections
            # output is [batch, num_detections, (x1, y1, x2, y2, class_id, score)] in normalized coordinates
            detections = detection_layer(proposals, mrcnn_class_probs, mrcnn_deltas, self.config)
            detection_bboxes = detections[:, :, :4]

            # Create masks for detections
            mrcnn_masks = self.mask(mrcnn_feature_maps, detection_bboxes)

            return detections, mrcnn_masks
